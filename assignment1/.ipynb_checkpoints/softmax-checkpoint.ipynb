{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=3):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.000000\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n",
      "numerical: 0.000000 analytic: 0.000000, relative error: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cs231n/gradient_check.py:125: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 0.000000e+00 computed in 0.000217s\n",
      "(500, 10)\n",
      "Shape of B: (500, 10)\n",
      "[ 0.09303389  0.0656999   0.07627055  0.08143969  0.07844851  0.09928006\n",
      "  0.07762391  0.10530111  0.07658465  0.09261611  0.08519344  0.09052487\n",
      "  0.11871067  0.09548016  0.10039122  0.09461763  0.09369817  0.09510884\n",
      "  0.08801499  0.1187794   0.09308979  0.0835963   0.08989898  0.06395062\n",
      "  0.1079137   0.10106201  0.12508618  0.09347146  0.08884917  0.11641519\n",
      "  0.10296985  0.09465992  0.13302869  0.09178488  0.08806187  0.08871545\n",
      "  0.09459917  0.09590318  0.08327882  0.11318269  0.08894924  0.10984417\n",
      "  0.09445072  0.11958151  0.12631812  0.09464711  0.11062587  0.09806885\n",
      "  0.07575706  0.10596574  0.07403545  0.07860308  0.09921374  0.09151212\n",
      "  0.08709929  0.10652041  0.11175114  0.09634237  0.07982901  0.07176225\n",
      "  0.12068437  0.09162072  0.11030675  0.11608463  0.08916666  0.11107061\n",
      "  0.12905319  0.10172581  0.10489736  0.09457817  0.11325678  0.09795472\n",
      "  0.10968301  0.19833467  0.09561601  0.07029905  0.11356188  0.08331082\n",
      "  0.10525985  0.09243979  0.06880321  0.10794532  0.0797274   0.13442875\n",
      "  0.08980904  0.11302852  0.08638689  0.09554492  0.08051846  0.10027628\n",
      "  0.058725    0.13064619  0.07163049  0.08111463  0.09152183  0.09300994\n",
      "  0.0955187   0.08564501  0.08813126  0.10508725  0.11302446  0.08304008\n",
      "  0.10168107  0.09760339  0.11229965  0.09348282  0.09013046  0.10190338\n",
      "  0.08935387  0.10693115  0.11502645  0.09040385  0.06984401  0.09752139\n",
      "  0.1301638   0.07977088  0.11599476  0.09233476  0.09718929  0.10143024\n",
      "  0.09173925  0.11005358  0.09485102  0.11044272  0.10217306  0.08698449\n",
      "  0.09505287  0.1144685   0.1190104   0.08165986  0.11531077  0.10207072\n",
      "  0.09077854  0.064465    0.14063311  0.09841496  0.10244482  0.09510627\n",
      "  0.12047088  0.10974178  0.07384281  0.0822624   0.09092383  0.13109462\n",
      "  0.08994344  0.10579761  0.0986766   0.10137853  0.12340367  0.09033984\n",
      "  0.104663    0.08678352  0.08762243  0.05211794  0.07465854  0.11280192\n",
      "  0.12667944  0.06788477  0.10539318  0.07768034  0.10228006  0.15056596\n",
      "  0.1231505   0.12051733  0.06940431  0.12074088  0.13020608  0.11408831\n",
      "  0.07872359  0.07781466  0.09650904  0.06553147  0.08501053  0.10708743\n",
      "  0.0946364   0.104307    0.09018999  0.07831355  0.11716962  0.0990676\n",
      "  0.09410764  0.09310669  0.09159738  0.10015376  0.1279032   0.07946055\n",
      "  0.12363266  0.15729728  0.10640737  0.09827015  0.08090945  0.05723105\n",
      "  0.15697207  0.08029397  0.08788724  0.10180218  0.08287158  0.09271919\n",
      "  0.06729007  0.09751422  0.09121738  0.11669731  0.07538289  0.08622159\n",
      "  0.11182062  0.11126707  0.10625438  0.0995151   0.11985942  0.09962467\n",
      "  0.09929538  0.09744402  0.10025592  0.09946976  0.07934777  0.10763609\n",
      "  0.14936877  0.09419076  0.10653152  0.09322852  0.09602635  0.10754493\n",
      "  0.10221261  0.1225562   0.08086338  0.1052929   0.08131418  0.11546665\n",
      "  0.14032809  0.09267898  0.07945304  0.1309196   0.10040583  0.10773883\n",
      "  0.14060249  0.10904607  0.080939    0.12027101  0.08336098  0.10214778\n",
      "  0.12319419  0.10180483  0.10229233  0.10045877  0.08849751  0.11485827\n",
      "  0.10196816  0.13359535  0.10062986  0.09094071  0.08586285  0.13113842\n",
      "  0.10951396  0.07748009  0.08344696  0.0877492   0.12093936  0.09116215\n",
      "  0.11158111  0.09112581  0.09837392  0.08800351  0.09568358  0.112092\n",
      "  0.09419117  0.10103088  0.12922192  0.12610037  0.08060873  0.07628074\n",
      "  0.09007945  0.09614279  0.08781557  0.09499106  0.18876785  0.06298706\n",
      "  0.11708308  0.17964353  0.10786921  0.07946563  0.06550195  0.1424422\n",
      "  0.12680243  0.07549347  0.08343623  0.11511581  0.11688691  0.08354312\n",
      "  0.09838747  0.11571399  0.08926572  0.1073626   0.11881175  0.06461944\n",
      "  0.10411502  0.07070748  0.09116263  0.10775961  0.10291772  0.0797777\n",
      "  0.10570685  0.13222224  0.10491862  0.09583185  0.06978132  0.09680296\n",
      "  0.10690668  0.07638552  0.10719522  0.11928698  0.08520331  0.06724101\n",
      "  0.15693032  0.08999561  0.08890024  0.0845771   0.10852128  0.10833875\n",
      "  0.10326395  0.10787431  0.11032798  0.11900156  0.08820478  0.09485634\n",
      "  0.11007313  0.08435387  0.09394607  0.09220599  0.07268459  0.10718334\n",
      "  0.11115821  0.07259066  0.07082301  0.12869186  0.08835984  0.10320505\n",
      "  0.11089102  0.14202337  0.08364131  0.15505058  0.09919495  0.10225073\n",
      "  0.08096675  0.12777285  0.08607864  0.10394717  0.11175791  0.11790813\n",
      "  0.10810779  0.09045041  0.09549523  0.09101496  0.13996362  0.09921714\n",
      "  0.10896917  0.10914582  0.09545512  0.10346537  0.0798083   0.10193604\n",
      "  0.0953455   0.08634359  0.12318978  0.10993439  0.0925922   0.1141427\n",
      "  0.06515105  0.11592139  0.11334062  0.09685884  0.09381286  0.0833451\n",
      "  0.08169026  0.11477103  0.10651587  0.12284989  0.11600951  0.0949856\n",
      "  0.11557016  0.10510808  0.11780026  0.1153903   0.0833856   0.09495356\n",
      "  0.12161665  0.1140705   0.08865848  0.11114953  0.16652754  0.09712475\n",
      "  0.11234867  0.11896717  0.10736621  0.10848791  0.10919794  0.08685642\n",
      "  0.10820845  0.11152516  0.12312936  0.07130168  0.09950872  0.11537246\n",
      "  0.11096989  0.11967395  0.07392987  0.07673248  0.10397389  0.09463642\n",
      "  0.08343485  0.10819639  0.09685787  0.09574467  0.10497675  0.09392751\n",
      "  0.09309458  0.1102756   0.0994384   0.09294211  0.09548935  0.10098979\n",
      "  0.10302947  0.09840113  0.08767014  0.11471884  0.09483863  0.09497852\n",
      "  0.08119987  0.0798305   0.09894234  0.09899166  0.10411158  0.1088309\n",
      "  0.09276961  0.09629265  0.12465281  0.10544284  0.09297894  0.12613698\n",
      "  0.12576939  0.0978523   0.09643203  0.13169787  0.07073402  0.0982896\n",
      "  0.09339138  0.07798495  0.09111033  0.10243442  0.07067537  0.11780632\n",
      "  0.10007766  0.10956185  0.12546026  0.08030506  0.15165773  0.10458477\n",
      "  0.09121021  0.13634235  0.09764363  0.08736291  0.130974    0.0769903\n",
      "  0.09805833  0.10245649  0.0742498   0.12702811  0.11459773  0.12409259\n",
      "  0.10623962  0.09991737  0.10427721  0.08829672  0.09075208  0.09990552\n",
      "  0.10210436  0.07510228  0.10547494  0.09815038  0.08561053  0.0912013\n",
      "  0.11112367  0.10063185  0.10310956  0.10242635  0.13537648  0.07961985\n",
      "  0.07912447  0.09268075  0.09984804  0.09443516  0.10390929  0.11421865\n",
      "  0.0968495   0.06145302  0.1026473   0.11185631  0.10542753  0.07998654\n",
      "  0.08929986  0.09873296]\n",
      "2.31616006171\n",
      "vectorized loss: 2.316160e+00 computed in 0.022582s\n",
      "Loss difference: 2.316160\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
